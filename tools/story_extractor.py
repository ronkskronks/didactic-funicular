#!/usr/bin/env python3
"""
ğŸ”¥ STORY EXTRACTOR - Transformador de LONG ASS STORIES em dados de treino
Converte as obras literÃ¡rias em formato conversacional para o Monster Chatbot
"""

import os
import re
import random

def extract_story_content(file_path):
    """Extrai conteÃºdo limpo dos arquivos markdown"""
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Remove headers e metadata
    lines = content.split('\n')
    clean_lines = []
    
    for line in lines:
        line = line.strip()
        # Pula linhas vazias, headers, e metadata
        if (line and 
            not line.startswith('#') and 
            not line.startswith('LIVRO') and
            not 'frases' in line.lower() and
            len(line) > 20):  # Apenas frases substanciais
            clean_lines.append(line)
    
    return clean_lines

def create_conversation_prompts():
    """Cria prompts para converter narrativa em conversa"""
    prompts = [
        "Me conte uma histÃ³ria",
        "Narre uma cena dramÃ¡tica",
        "Descreva um momento emocional",
        "Como essa histÃ³ria continua?",
        "Conte-me sobre as emoÃ§Ãµes da personagem",
        "O que aconteceu em seguida?",
        "Desenvolva essa narrativa",
        "Elabore sobre esse sentimento",
        "Continue a histÃ³ria",
        "Descreva a prÃ³xima cena",
        "Me fale sobre essa situaÃ§Ã£o",
        "Narre esse momento",
        "Como a protagonista se sentia?",
        "O que se passou pela mente dela?",
        "Descreva o ambiente da cena",
        "Conte sobre esse episÃ³dio",
        "Elabore essa passagem",
        "Desenvolva esse momento",
        "Como essa cena se desenrolou?",
        "Fale sobre essa experiÃªncia emocional"
    ]
    return prompts

def group_sentences_into_chunks(sentences, chunk_size=3):
    """Agrupa frases em chunks para respostas mais longas"""
    chunks = []
    for i in range(0, len(sentences), chunk_size):
        chunk = sentences[i:i+chunk_size]
        # Junta as frases do chunk
        combined = ' '.join(chunk)
        chunks.append(combined)
    return chunks

def create_dialogue_dataset(story_files, output_file):
    """Cria dataset conversacional a partir das histÃ³rias"""
    print("ğŸ“š Extraindo dados das LONG ASS STORIES...")
    
    all_sentences = []
    prompts = create_conversation_prompts()
    
    # Extrai todas as frases de todos os livros
    for file_path in story_files:
        print(f"   ğŸ“– Processando: {os.path.basename(file_path)}")
        sentences = extract_story_content(file_path)
        all_sentences.extend(sentences)
        print(f"      âœ“ {len(sentences)} frases extraÃ­das")
    
    print(f"\nğŸ“Š Total de frases coletadas: {len(all_sentences)}")
    
    # Agrupa frases em chunks para respostas mais elaboradas
    story_chunks = group_sentences_into_chunks(all_sentences, chunk_size=3)
    print(f"ğŸ“¦ Criados {len(story_chunks)} chunks narrativos")
    
    # Cria pares pergunta-resposta
    dialogue_pairs = []
    
    # 1. Conversas sobre histÃ³rias
    for i, chunk in enumerate(story_chunks[:500]):  # Limita para nÃ£o explodir
        prompt = random.choice(prompts)
        dialogue_pairs.append((prompt, chunk))
    
    # 2. Conversas sequenciais (continuaÃ§Ã£o de narrativa)
    for i in range(min(200, len(story_chunks) - 1)):
        prompt = f"Continue essa histÃ³ria: {story_chunks[i][:100]}..."
        response = story_chunks[i + 1]
        dialogue_pairs.append((prompt, response))
    
    # 3. AnÃ¡lise de emoÃ§Ãµes (extrai emoÃ§Ãµes das frases)
    emotions = ['tristeza', 'alegria', 'raiva', 'medo', 'amor', 'esperanÃ§a', 
              'ansiedade', 'tÃ©dio', 'luto', 'ambiÃ§Ã£o', 'desejo', 'estagnaÃ§Ã£o']
    
    for emotion in emotions:
        emotion_sentences = [s for s in all_sentences if emotion in s.lower()]
        if emotion_sentences:
            sample_sentences = random.sample(emotion_sentences, min(10, len(emotion_sentences)))
            for sentence in sample_sentences:
                prompt = f"Me conte sobre {emotion}"
                dialogue_pairs.append((prompt, sentence))
    
    # 4. Personagem (Lia)
    lia_sentences = [s for s in all_sentences if 'Lia' in s or 'ela' in s]
    for i in range(min(100, len(lia_sentences))):
        prompts_lia = [
            "Fale sobre a protagonista Lia",
            "O que aconteceu com Lia?",
            "Descreva Lia nessa situaÃ§Ã£o",
            "Como Lia se sentia?",
            "Continue a histÃ³ria de Lia"
        ]
        prompt = random.choice(prompts_lia)
        dialogue_pairs.append((prompt, lia_sentences[i]))
    
    print(f"ğŸ’¬ Criados {len(dialogue_pairs)} pares de diÃ¡logo")
    
    # Salva no formato USER/BOT
    with open(output_file, 'w', encoding='utf-8') as f:
        for prompt, response in dialogue_pairs:
            f.write(f"USER: {prompt}\n")
            f.write(f"BOT: {response}\n")
            f.write("\n")
    
    print(f"âœ… Dataset salvo em: {output_file}")
    return len(dialogue_pairs)

def enhance_existing_dataset(original_file, story_file, output_file):
    """Combina dataset original com dados das histÃ³rias"""
    print("ğŸ”— Combinando datasets...")
    
    # LÃª dataset original
    with open(original_file, 'r', encoding='utf-8') as f:
        original_data = f.read()
    
    # LÃª dados das histÃ³rias
    with open(story_file, 'r', encoding='utf-8') as f:
        story_data = f.read()
    
    # Combina
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("# MEGA DATASET - Original + LONG ASS STORIES\n\n")
        f.write("# === CONVERSAS ORIGINAIS ===\n")
        f.write(original_data)
        f.write("\n\n# === NARRATIVAS Ã‰PICAS ===\n")
        f.write(story_data)
    
    print(f"âœ… Mega dataset criado: {output_file}")

def main():
    print("ğŸ”¥ğŸ¤– STORY EXTRACTOR - ALIMENTANDO O MONSTER! ğŸ¤–ğŸ”¥")
    print("=" * 55)
    
    # Localiza arquivos das histÃ³rias
    story_dir = "FRESHLY_MILKED_DATA"
    story_files = []
    
    for filename in os.listdir(story_dir):
        if filename.endswith('.md') and 'trilogia' in filename:
            story_files.append(os.path.join(story_dir, filename))
    
    print(f"ğŸ“š Encontrados {len(story_files)} livros da trilogia:")
    for file in story_files:
        print(f"   ğŸ“– {os.path.basename(file)}")
    
    # Cria dataset de histÃ³rias
    story_dataset = "data/story_conversations.txt"
    num_conversations = create_dialogue_dataset(story_files, story_dataset)
    
    # Combina com dataset original
    original_dataset = "data/chatbot_training.txt"
    mega_dataset = "data/mega_training_dataset.txt"
    enhance_existing_dataset(original_dataset, story_dataset, mega_dataset)
    
    print("\nğŸ‰ EXTRAÃ‡ÃƒO CONCLUÃDA!")
    print(f"ğŸ“Š EstatÃ­sticas:")
    print(f"   ğŸ’¬ {num_conversations} conversas sobre histÃ³rias")
    print(f"   ğŸ“š {len(story_files)} livros processados")
    print(f"   ğŸš€ Mega dataset pronto para o Monster!")
    print("\nğŸ”¥ Agora Ã© sÃ³ treinar o Monster com esse BANQUETE!")

if __name__ == "__main__":
    main()